# Fine-Tuning Models

This repository Python notebooks demonstrating the fine-tuning of some pre-trained models. The project explores both standard fine-tuning and a parameter-efficient fine-tuning approach using Low-Rank Adaptation (LoRA). Moreover, with Llama we explore model quantization.

NOTE: the llama models' code was only run on Colab due to GPU requirements.

## Project Overview

- **Model_1**: `bert-base-uncased` (a pre-trained BERT model)
- **Model_2**: `llama-3-8B` (a pre-trained Llama model, 2024)
- **Model_3**: `llama-3-8B-instruct` (a pre-trained and fine-tuned Llama model, 2024)
- **Dataset_1**: Stanford IMDB reviews (for binary sentiment classification)
- **Dataset_2**: English Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.
- **Libraries**: torch, huggingface_hub, transformers, accelerate, bitsandbytes, peft

## Key Features

- **Tokenization**: Converts raw text into tokens using associated tokenizer.
- **Standard Fine-Tuning**: Updates all model parameters using AdamW optimizer with carefully selected hyperparameters.
- **LoRA Fine-Tuning**: Efficiently fine-tunes the model by injecting trainable low-rank matrices into modules of the Transformer layers.
- **Quantization**: The process of discretizing an input from a representation that holds more information to a representation with less information.

## Files

1. `bert_fine_tuning.ipynb`: a pre-trained BERT model finetuned on the Stanford IMDB reviews dataset, both with standard ft and with LoRA.
2. `lama3_8b_ft_sa_final.ipynb`: llama-3 pretrained model finetuned on the same Stanford IMDB reviews dataset, using QLoRA.
3. `llama3_8b_instruct_ft.ipynb`: llama-3 instruct model finetuned on text generation, using QLoRA.
4. `requirements.txt`: List of required packages and dependencies.

