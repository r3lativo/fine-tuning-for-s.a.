{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT on Sentiment Analysis task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "In this notebook, we will demonstrate the process of fine-tuning a pre-trained language model, specifically `bert-base-uncased`, for sentiment analysis on the Stanford IMDB reviews dataset. Fine-tuning is a critical step in transferring the knowledge learned by a large pre-trained model, such as BERT, to a specific downstream task like sentiment classification. By applying fine-tuning techniques, we can adapt the model to our dataset and achieve higher performance with fewer labeled examples than training from scratch.\n",
    "\n",
    "The BERT model (`bert-base-uncased`) we are using is a Transformer-based model that has been pre-trained on a large corpus of text in an unsupervised manner. This model has proven to be highly effective in various NLP tasks. Our dataset, the Stanford IMDB reviews, is a well-known benchmark for sentiment analysis, consisting of movie reviews labeled as positive or negative.\n",
    "\n",
    "We will work on a smaller version of the dataset (1000 examples) to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from datasets import load_dataset  # Huggingface datasets\n",
    "from transformers import (  # Huggingface transformers\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "import torch  # Cuda access\n",
    "import evaluate  # Evaluate the model\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main settings\"\"\"\n",
    "# Check if cuda is present\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Loading the IMDB movie reviews dataset\n",
    "# This dataset is used for binary sentiment classification (positive/negative).\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# Set model\n",
    "model_checkpoint = 'bert-base-uncased'\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Before feeding text data into BERT, it must be tokenized into a format that the model can understand. Tokenization is the process of converting raw text into tokens, which are then mapped to integers that correspond to the model's vocabulary. BERT uses WordPiece tokenization, which splits words into subwords to handle the vast variety of words in natural language more efficiently. This allows BERT to handle out-of-vocabulary words by breaking them down into known subword tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Helper function to tokenize text from the Dataset\"\"\"\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    # Tokenize and truncate\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "        )\n",
    "    \n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the dataset and store it as torch tensor\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller version of the dataset, to speed up the training\n",
    "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive to total ratio in train: 48.8%\n",
      "Positive to total ratio in eval:  48.8%\n"
     ]
    }
   ],
   "source": [
    "# Check the Positive to total ratio for the shuffled smaller datasets\n",
    "a = sum(small_train_dataset['label'])/len(small_train_dataset['label'])\n",
    "b = sum(small_eval_dataset['label'])/len(small_eval_dataset['label'])\n",
    "\n",
    "print(f\"Positive to total ratio in train: {a*100}%\")\n",
    "print(f\"Positive to total ratio in eval:  {b*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label maps, to be able to pass from labels to id and vice versa\n",
    "id2label = {0: \"NEG\", 1: \"POS\"}\n",
    "label2id = {\"NEG\":0, \"POS\":1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collation with Padding\n",
    "\n",
    "When working with batches of data, each input in a batch must have the same length. `DataCollatorWithPadding` is used to ensure that all sequences in a batch are padded to the same length, which is necessary because different reviews have varying lengths. This padding is dynamically applied based on the longest sequence in each batch, making the process more memory-efficient than static padding to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Fine-Tuning\n",
    "\n",
    "In standard fine-tuning, all the parameters of the pre-trained model are updated during the training process. This means that every layer in BERT will be adjusted based on the gradients calculated from the loss function. This is obviously computationally expensive, and overfitting is around the corner, especially when working with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model to device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_slice):\n",
    "    \"\"\"Helper function to test the model on small set of sentences from data\"\"\"\n",
    "\n",
    "    print(\"Model predictions:\")\n",
    "    print(f\"|Text{' '*50}|Gold |Pred |\")\n",
    "    print(f\"{'-'*50}\")\n",
    "\n",
    "    correct_count = 0\n",
    "\n",
    "    for e in data_slice:\n",
    "        text = e['text']\n",
    "        label = id2label[e['label']]\n",
    "\n",
    "        # Tokenize text\n",
    "        inputs = tokenizer.encode(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "        \n",
    "        # Compute logits\n",
    "        with torch.no_grad(): logits = model(inputs).logits\n",
    "        # Convert logits to label\n",
    "        predictions = torch.argmax(logits)\n",
    "\n",
    "        l_pred = id2label[predictions.tolist()]\n",
    "        print(f\"|{text[:50]}... |{label}  |{l_pred}  |\")\n",
    "        \n",
    "        # Keep track of correct prediction for accuracy\n",
    "        if label == l_pred: correct_count += 1\n",
    "\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"Accuracy: {(correct_count/10)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions:\n",
      "|Text                                                  |Gold |Pred |\n",
      "--------------------------------------------------\n",
      "|I watched this movie which I really thought had a ... |NEG  |NEG  |\n",
      "|This movie is perfect for any aspiring screen writ... |NEG  |NEG  |\n",
      "|I'm a Boorman fan, but this is arguably his least ... |NEG  |POS  |\n",
      "|I wonder why I haven't heard of this movie before.... |POS  |POS  |\n",
      "|Finally! Third time lucky. This film has been alwa... |POS  |POS  |\n",
      "|Bloody awful! There's just no other way to put it.... |NEG  |NEG  |\n",
      "|There are so very few films where just the title t... |NEG  |POS  |\n",
      "|Chaplin is a doughboy in his final film of 1918, a... |POS  |POS  |\n",
      "|\"Black Dragons\" is a second feature WWII propagand... |NEG  |POS  |\n",
      "|By Randolph Scott standards of the 1950s, this is ... |NEG  |NEG  |\n",
      "--------------------------------------------------\n",
      "Accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "# Test model on ten random sentences\n",
    "example_data = small_eval_dataset.shuffle().select(range(10))\n",
    "test_model(model, example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"Helper function to evaluate the trainer\"\"\"\n",
    "    #print(p)\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    return {\"accuracy\": accuracy.compute(predictions=pred, references=labels)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters\n",
    "\n",
    "The performance of the fine-tuning process is influenced by several hyperparameters:\n",
    "\n",
    "- **Learning Rate (`lr`)**: controls the step size of the optimizer, i.e., how much to update the model's weights during each iteration.\n",
    "- **Batch Size (`batch_size`)**: refers to the number of samples processed before the model’s weights are updated. A smaller batch size can lead to more stable training, especially on smaller datasets.\n",
    "- **Number of Epochs (`num_epochs`)**: the number of complete passes through the training dataset.\n",
    "- **Optimizer (`optimizer`)**: we use `adamw_torch`, a variant of the Adam optimizer that implements weight decay (a form of regularization). AdamW helps prevent overfitting by penalizing large weights in the model, thus improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "optimizer = 'adamw_torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"bert_finetuned\",\n",
    "    learning_rate=lr,\n",
    "    optim=optimizer,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 52:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.756400</td>\n",
       "      <td>0.898487</td>\n",
       "      <td>{'accuracy': 0.488}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.730700</td>\n",
       "      <td>0.716874</td>\n",
       "      <td>{'accuracy': 0.488}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.692901</td>\n",
       "      <td>{'accuracy': 0.512}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.7255193684895833, metrics={'train_runtime': 3133.4239, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.12, 'total_flos': 732821844720960.0, 'train_loss': 0.7255193684895833, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions:\n",
      "|Text                                                  |Gold |Pred |\n",
      "--------------------------------------------------\n",
      "|I'm a Jean Harlow fan, because she had star qualit... |NEG  |NEG  |\n",
      "|Can you say \"All shock, no plot?\" There were so ma... |NEG  |NEG  |\n",
      "|A wonderful, free flowing, often lyrical film that... |POS  |NEG  |\n",
      "|Errol Flynn is \"Gentleman Jim\" in this 1942 film a... |POS  |NEG  |\n",
      "|This movie has taken a lot of stick. It was slated... |POS  |NEG  |\n",
      "|A bondage, humiliation, S&M show, and not much els... |NEG  |NEG  |\n",
      "|To put it simply, I am not fond of westerns. And h... |NEG  |NEG  |\n",
      "|John Carpenter's Halloween is quite frankly a horr... |POS  |NEG  |\n",
      "|I recently found a copy for $5 at a video store, a... |POS  |NEG  |\n",
      "|When a film has no fewer than FIVE different title... |NEG  |NEG  |\n",
      "--------------------------------------------------\n",
      "Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Empirical test after fine-tuning\n",
    "test_model(model, example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally save model\n",
    "The model can also be saved to local in order to be able to call it back and reuse it once trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save model\n",
    "#trainer.save_model('custom_bert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to call the model back once saved\n",
    "#model_2 = AutoModelForSequenceClassification.from_pretrained('custom_bert')\n",
    "#model_2.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT and LoRA\n",
    "\n",
    "PEFT (Parameter-Efficient Fine-Tuning) refers to methods that aim to fine-tune models with a smaller number of trainable parameters. This is particularly useful for large models like BERT, where full fine-tuning can be impractical due to the high computational cost.\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is an efficient fine-tuning technique designed to reduce the computational and memory costs associated with standard fine-tuning. Instead of updating all the parameters of the pre-trained model, LoRA introduces **trainable low-rank matrices** into the layers of the model.\n",
    "\n",
    "The reason why LoRA works so well is because the new trainable parameters (matrices) are inserted in a way that allows the model to capture task-specific information without modifying the original pre-trained weights, and their dimension is drastically reduced in relation to the rank we assign when calling LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up space\n",
    "del model\n",
    "del trainer\n",
    "if device == 'cuda': torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "- `task_type`: \"SEQ_CLS\" specifies that the task is sequence classification, which is appropriate for sentiment analysis.\n",
    "- `r`: the rank of the low-rank matrices added by LoRA.\n",
    "- `lora_alpha`: a scaling factor for the low-rank matrices, controlling the impact of the LoRA weights on the model.\n",
    "- `lora_dropout`: introduces dropout to the LoRA layers, helping to prevent overfitting.\n",
    "- `target_modules`: specifies to which modules within the Transformers layers LoRA will be applied. In our case, the `query` and `key` modules, which are key components in the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT configuration parameters with LoRA\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules=['query', 'key']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of `target_modules`\n",
    "\n",
    "In the Transformer architecture, which underlies BERT, attention mechanisms are central to the model's ability to understand and process language. The attention mechanism is driven by three main components: `query`, `key`, and `value` vectors. These components are derived from the input embeddings and are used to calculate the attention scores that determine how much focus each token should pay to other tokens in a sequence.\n",
    "\n",
    "- **Query**: Represents the input's intent or question—essentially, \"what am I looking for?\"\n",
    "- **Key**: Represents the properties of the other tokens—\"what do I have that might be relevant?\"\n",
    "- **Value**: Represents the actual information or content that will be attended to, based on the similarity between the query and key.\n",
    "\n",
    "The attention score for each pair of tokens is computed as the dot product between the `query` vector of one token and the `key` vector of another. This score determines the weight or importance of the corresponding `value` vectors in the final output. Thus, the `query` and `key` components are critical in shaping how information flows through the model and how relationships between tokens are modeled.\n",
    "\n",
    "By focusing on the `query` and `key` modules in LoRA, we are effectively modifying the way the model calculates attention scores. For sentiment analysis, understanding the relationships between words (e.g., negations like \"not\" and sentiment-laden words like \"bad\") is crucial. By fine-tuning the `query` and `key` matrices, LoRA allows the model to learn how to better focus on these relationships in the context of sentiment classification.\n",
    "\n",
    "Moreover, by restricting modifications to the `query` and `key` modules, LoRA preserves much of this general knowledge encoded in the `value` vectors, while still allowing for the task-specific adjustments necessary for sentiment analysis. This targeted approach allows for efficient and effective fine-tuning, making it a preferred strategy when computational resources are limited or when working with smaller datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=4, target_modules={'key', 'query'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the PEFT configuration\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# We can see how much of the total parameters are trainable for the given model\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertForSequenceClassification(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters (same as before)\n",
    "lr = 1e-3\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "optimizer = 'adamw_torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"bert_finetuned_lora\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 44:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.376127</td>\n",
       "      <td>{'accuracy': 0.857}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.394400</td>\n",
       "      <td>0.394746</td>\n",
       "      <td>{'accuracy': 0.85}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.478307</td>\n",
       "      <td>{'accuracy': 0.878}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.40438668060302735, metrics={'train_runtime': 2677.911, 'train_samples_per_second': 1.12, 'train_steps_per_second': 0.14, 'total_flos': 734096665991808.0, 'train_loss': 0.40438668060302735, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions:\n",
      "|Text                                                  |Gold |Pred |\n",
      "--------------------------------------------------\n",
      "|I'm a Jean Harlow fan, because she had star qualit... |NEG  |POS  |\n",
      "|Can you say \"All shock, no plot?\" There were so ma... |NEG  |NEG  |\n",
      "|A wonderful, free flowing, often lyrical film that... |POS  |POS  |\n",
      "|Errol Flynn is \"Gentleman Jim\" in this 1942 film a... |POS  |POS  |\n",
      "|This movie has taken a lot of stick. It was slated... |POS  |POS  |\n",
      "|A bondage, humiliation, S&M show, and not much els... |NEG  |NEG  |\n",
      "|To put it simply, I am not fond of westerns. And h... |NEG  |NEG  |\n",
      "|John Carpenter's Halloween is quite frankly a horr... |POS  |POS  |\n",
      "|I recently found a copy for $5 at a video store, a... |POS  |POS  |\n",
      "|When a film has no fewer than FIVE different title... |NEG  |NEG  |\n",
      "--------------------------------------------------\n",
      "Accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "test_model(model, example_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
